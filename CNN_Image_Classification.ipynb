{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "samaneh.CW1",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdnsT8VYCSYF",
        "colab_type": "text"
      },
      "source": [
        "## COMP5623 Coursework on Image Classification with Convolutional Neural Networks \n",
        "\n",
        "Starter code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKS20wjsCSYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from  torch.utils.data import Dataset\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from skimage import io, transform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import math\n",
        "import cv2\n",
        "import timeit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UiYzwkAiWbw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpRGGlofCSYM",
        "colab_type": "text"
      },
      "source": [
        "### Part I\n",
        "\n",
        "The first part of the assignment is to build a CNN and train it on a subset of the ImageNet dataset. We will first create a dataframe with all the references to the images and their labels.\n",
        "\n",
        "To download the images into your work environment, clone into a git respository containing the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqhQboSCCSYN",
        "colab_type": "code",
        "outputId": "f632623b-9a19-45df-d68f-fdbe11cfcc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! git clone https://github.com/MohammedAlghamdi/imagenet10.git"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'imagenet10' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8edZnYShg7M1",
        "colab_type": "text"
      },
      "source": [
        "Check that the repository is there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2RIZGHaCVAC",
        "colab_type": "code",
        "outputId": "78ef321f-2494-4fe5-d5d6-1f98b507f17a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! ls"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imagenet10  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47tv-wIQCTcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir = \"imagenet10/train_set/\"\n",
        "class_names = [\n",
        "  \"baboon\",\n",
        "  \"banana\",\n",
        "  \"canoe\",\n",
        "  \"cat\",\n",
        "  \"desk\",\n",
        "  \"drill\",\n",
        "  \"dumbbell\",\n",
        "  \"football\",\n",
        "  \"mug\",\n",
        "  \"orange\",\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6oMdWl5CSYR",
        "colab_type": "text"
      },
      "source": [
        "A helper function for reading in images and assigning labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQfcD3jyCSYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_meta(root_dir, dirs):\n",
        "    \"\"\" Fetches the meta data for all the images and assigns labels.\n",
        "    \"\"\"\n",
        "    paths, classes = [], []\n",
        "    for i, dir_ in enumerate(dirs):\n",
        "        for entry in os.scandir(root_dir + dir_):\n",
        "            if (entry.is_file()):\n",
        "                paths.append(entry.path)\n",
        "                classes.append(i)\n",
        "                \n",
        "    return paths, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J967KW0CSYX",
        "colab_type": "text"
      },
      "source": [
        "Now we create a dataframe using all the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gDPs1NCCSYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Benign images we will assign class 0, and malignant as 1\n",
        "paths, classes = get_meta(root_dir, class_names)\n",
        "\n",
        "data = {\n",
        "    'path': paths,\n",
        "    'class': classes\n",
        "}\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=['path', 'class'])\n",
        "data_df = data_df.sample(frac=1).reset_index(drop=True) # Shuffles the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRUDj3WihItY",
        "colab_type": "text"
      },
      "source": [
        "View some sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK2GPzfVCSYc",
        "colab_type": "code",
        "outputId": "20046921-22cd-4869-ef50-86835d6b4800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(\"Found\", len(data_df), \"images.\")\n",
        "data_df.head()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9000 images.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>imagenet10/train_set/dumbbell/n03255030_11306....</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>imagenet10/train_set/dumbbell/n03255030_8667.JPEG</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>imagenet10/train_set/banana/n07753592_8698.JPEG</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>imagenet10/train_set/cat/n02123159_7316.JPEG</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>imagenet10/train_set/dumbbell/n03255030_9844.JPEG</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                path  class\n",
              "0  imagenet10/train_set/dumbbell/n03255030_11306....      6\n",
              "1  imagenet10/train_set/dumbbell/n03255030_8667.JPEG      6\n",
              "2    imagenet10/train_set/banana/n07753592_8698.JPEG      1\n",
              "3       imagenet10/train_set/cat/n02123159_7316.JPEG      3\n",
              "4  imagenet10/train_set/dumbbell/n03255030_9844.JPEG      6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai3uvziWCSYh",
        "colab_type": "text"
      },
      "source": [
        "Now we will create the Dataset class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyUb-rzQCSYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageNet10(Dataset):\n",
        "    \"\"\" ImageNet10 dataset. \"\"\"\n",
        "\n",
        "    def __init__(self, df, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (string): Directory with all the images\n",
        "            df (DataFrame object): Dataframe containing the images, paths and classes\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load image from path and get label\n",
        "        x = Image.open(self.df['path'][index])\n",
        "        try:\n",
        "          x = x.convert('RGB') # To deal with some grayscale images in the data\n",
        "        except:\n",
        "          pass\n",
        "        y = torch.tensor(int(self.df['class'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqtRjBozCSYk",
        "colab_type": "text"
      },
      "source": [
        "Compute what we should normalise the dataset to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPqfMPuZCSYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_img_mean_std(image_paths):\n",
        "    \"\"\"\n",
        "        Author: @xinruizhuang. Computing the mean and std of three channel on the whole dataset,\n",
        "        first we should normalize the image from 0-255 to 0-1\n",
        "    \"\"\"\n",
        "\n",
        "    img_h, img_w = 224, 224\n",
        "    imgs = []\n",
        "    means, stdevs = [], []\n",
        "\n",
        "    for i in tqdm(range(len(image_paths))):\n",
        "        img = cv2.imread(image_paths[i])\n",
        "        img = cv2.resize(img, (img_h, img_w))\n",
        "        imgs.append(img)\n",
        "\n",
        "    imgs = np.stack(imgs, axis=3)\n",
        "    print(imgs.shape)\n",
        "\n",
        "    imgs = imgs.astype(np.float32) / 255.\n",
        "\n",
        "    for i in range(3):\n",
        "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
        "        means.append(np.mean(pixels))\n",
        "        stdevs.append(np.std(pixels))\n",
        "\n",
        "    means.reverse()  # BGR --> RGB\n",
        "    stdevs.reverse()\n",
        "\n",
        "    print(\"normMean = {}\".format(means))\n",
        "    print(\"normStd = {}\".format(stdevs))\n",
        "    return means, stdevs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_1KSGUe4sEW",
        "colab_type": "code",
        "outputId": "c568440f-e36a-4c41-f52a-6406861f7ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "norm_mean, norm_std = compute_img_mean_std(paths)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 2292/9000 [00:13<00:50, 132.20it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqrG3FCUCSYo",
        "colab_type": "text"
      },
      "source": [
        "Now let's create the transforms to normalise and turn our data into tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVeZpgM-CSYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(norm_mean, norm_std),\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAPzCfr2CSYt",
        "colab_type": "text"
      },
      "source": [
        "Let's split the data into train and test sets and instantiate our new ISIC_Dataset objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POCexxXjCSYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_split = 0.70 # Defines the ratio of train/valid/test data.\n",
        "valid_split = 0.10\n",
        "\n",
        "train_size = int(len(data_df)*train_split)\n",
        "valid_size = int(len(data_df)*valid_split)\n",
        "\n",
        "ins_dataset_train = ImageNet10(\n",
        "    df=data_df[:train_size],\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "ins_dataset_valid = ImageNet10(\n",
        "    df=data_df[train_size:(train_size + valid_size)].reset_index(drop=True),\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "ins_dataset_test = ImageNet10(\n",
        "    df=data_df[(train_size + valid_size):].reset_index(drop=True),\n",
        "    transform=data_transform,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzVfNzvmhTGJ",
        "colab_type": "text"
      },
      "source": [
        "You will need to create DataLoaders for the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_womtSmIhgSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_train,\n",
        "    batch_size=5,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_test,\n",
        "    batch_size=12, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_valid,\n",
        "    batch_size=5, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "classes = range(0, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X00vuzPWhY5h",
        "colab_type": "text"
      },
      "source": [
        "A framework for the ConvNet model, missing all layers except the final fully-connected layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B5x07o3CSY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional neural network\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "      super(ConvNet, self).__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(3,16,4)\n",
        "      self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "      self.conv2 = nn.Sequential(\n",
        "          nn.Conv2d(16,32,4),\n",
        "          nn.MaxPool2d(2,2),\n",
        "          nn.ReLU(), \n",
        "          nn.Dropout(0.1)\n",
        "      )  \n",
        "\n",
        "      self.conv3 = nn.Sequential(\n",
        "          nn.Conv2d(32,64,4),\n",
        "          nn.MaxPool2d(2,2),\n",
        "          nn.ReLU(), \n",
        "          nn.Dropout(0.1)\n",
        "      )\n",
        "\n",
        "      self.conv4 = nn.Sequential(\n",
        "          nn.Conv2d(64,128,4),\n",
        "          nn.MaxPool2d(2,2),\n",
        "          nn.ReLU(), \n",
        "       #   nn.Dropout(0.1)\n",
        "      )\n",
        "\n",
        "      self.conv5 = nn.Sequential(\n",
        "          nn.Conv2d(128,128,4),\n",
        "          nn.MaxPool2d(2,2),\n",
        "          nn.ReLU(), \n",
        "          #nn.Dropout(0.1)\n",
        "      )\n",
        "\n",
        "\n",
        "      self.fc1 = nn.Linear(3200, 512)       \n",
        "      self.fc2 = nn.Linear(512, num_classes)\n",
        "      \n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        # Complete the graph\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
        "        x = torch.nn.functional.dropout(x, p= 0.1)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E30EMmLh2KkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44VE2TrD2T04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device configuration - defaults to CPU unless GPU is available on device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yunzD_Ph2bhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ConvNet().to(device)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UtD6u-LGDVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Stochastic gradient descent\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0003, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Je-8Vs0P7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_list = []\n",
        "loss_list = []\n",
        "total = 0\n",
        "correct = 0\n",
        "correctv = 0\n",
        "totalv = 0\n",
        "val_acc = []\n",
        "val_loss_list = []\n",
        "def train_model_epochs(num_epochs):\n",
        "    \"\"\" Copy of function train_model_epochs but explicitly copying data to device \n",
        "        during training. \n",
        "    \"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        running_loss = 0.0\n",
        "        batch_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            images, labels = data\n",
        "\n",
        "            # Explicitly specifies that data is to be copied onto the device!\n",
        "            images = images.to(device)  # <----------- And note it's NOT an in-place operation; original\n",
        "            labels = labels.to(device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "           # running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            global correct, total, acc_list, loss_list\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Print our loss\n",
        "            running_loss += loss.item()\n",
        "            batch_loss += loss.item()\n",
        "            if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "                print('Epoch / Batch [%d / %d] - Loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 1000))\n",
        "                running_loss = 0.0\n",
        "            if i == len(train_loader) - 1:\n",
        "              acc_list.append(correct / total)\n",
        "              loss_list.append(batch_loss / len(train_loader))\n",
        "              batch_loss = 0.0\n",
        " # train_model_epochs(num_epochs)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          val_loss = 0.0\n",
        "          for data in valid_loader:\n",
        "              imagesv, labelsv = data\n",
        "\n",
        "              imagesv = imagesv.to(device)  \n",
        "              labelsv = labelsv.to(device) \n",
        "              \n",
        "              outputsv = model(imagesv)\n",
        "              lossv = criterion(outputsv, labelsv)\n",
        "\n",
        "              val_loss += lossv.item()\n",
        "              \n",
        "              _, predictedv = torch.max(outputsv.data, 1)\n",
        "              \n",
        "              global val_acc,val_loss_list, totalv, correctv\n",
        "              totalv += labelsv.size(0)\n",
        "              correctv += (predictedv == labelsv).sum().item()\n",
        "              \n",
        "\n",
        "        print('val_loss: %.3f - val_acc:%.3f' % (val_loss / len(valid_loader), (correct / total)))\n",
        "        val_acc.append(correctv / totalv)\n",
        "        val_loss_list.append(val_loss / len(valid_loader))\n",
        "        val_loss = 0.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYJgw-KQ6Xh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cpu_train_time = timeit.timeit(\n",
        "    \"train_model_epochs(num_epochs)\",\n",
        "    setup=\"num_epochs=10\",\n",
        "    number=1,\n",
        "    globals=globals(),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YS7w0C-6f8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cpu_train_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfjeEMDrmaOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(len(acc_list))\n",
        "nb_epochs = len(epochs)\n",
        "\n",
        "f2 = plt.figure(2)\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis((0, nb_epochs, 0, 1.2))\n",
        "plt.plot(epochs, acc_list, 'bo', label = 'training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'validation acc')\n",
        "plt.title('train and validation acc')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis((0, nb_epochs, 0, 5))\n",
        "plt.plot(epochs, loss_list, 'bo', label = 'training loss')\n",
        "plt.plot(epochs, val_loss_list, 'b', label = 'validation loss')\n",
        "plt.title('train and validation loss')\n",
        "plt.legend()\n",
        "plt.draw()\n",
        "plt.pause(0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25lNUcupJ85n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "labels_list = []\n",
        "predicted_list = []\n",
        "\n",
        "# Why don't we need gradients? What happens if we do include gradients?\n",
        "with torch.no_grad():\n",
        "    \n",
        "    # Iterate over the test set\n",
        "    for data in test_loader:\n",
        "\n",
        "      images, labels = data\n",
        "      # Explicitly specifies that data is to be copied onto the device!\n",
        "      images = images.to(device)  # <----------- And note it's NOT an in-place operation; original\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "        \n",
        "      # torch.max is an argmax operation\n",
        "      \n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      labels_list += labels.tolist()\n",
        "      predicted_list += predicted.tolist()\n",
        "\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joF2QfT-03un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "cm = confusion_matrix(torch.as_tensor(labels_list), torch.as_tensor(predicted_list))\n",
        "\n",
        "cm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvzxJw8wyNRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          classes,\n",
        "                          normalize=True,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix very prettily.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "\n",
        "    # Specify the tick marks and axis text\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    # The data formatting\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    \n",
        "    # Print the text of the matrix, adjusting text colour for display\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6dpXfL25JlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(cm, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9nFO0P7nsF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convL = model.conv1\n",
        "kernels = convL.weight.data\n",
        "kernels = kernels.to(\"cpu\")\n",
        "kernelNP = kernels.numpy()\n",
        "\n",
        "filterNumber = 4\n",
        "picID = 1\n",
        "\n",
        "for k in range(0, 16):\n",
        "  graph = plt.subplot(filterNumber, 4, k+1)\n",
        "  graph.set_xticks([])\n",
        "  graph.set_yticks([])\n",
        "  plt.imshow(np.clip(kernels[k], 0, 1)[:,:,0], cmap ='gray') # 0-2 = RGB red \n",
        "  picID+=1\n",
        "plt.show()\n",
        "\n",
        "for k in range(0, 16):\n",
        "  graph = plt.subplot(filterNumber, 4, k+1)\n",
        "  graph.set_xticks([])\n",
        "  graph.set_yticks([])\n",
        "  plt.imshow(np.clip(kernels[k], 0, 1)[:,:,1], cmap ='gray') # 0-2 = RGB green\n",
        "  picID+=1\n",
        "plt.show()\n",
        "\n",
        "for k in range(0, 16):\n",
        "  graph = plt.subplot(filterNumber, 4, k+1)\n",
        "  graph.set_xticks([])\n",
        "  graph.set_yticks([])\n",
        "  plt.imshow(np.clip(kernels[k], 0, 1)[:,:,2], cmap ='gray') # 0-2 = RGB blue\n",
        "  picID+=1\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMh7X2TE9CwG",
        "colab_type": "text"
      },
      "source": [
        "Feature Maps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn4cSSI3ntds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    # Iterate over the test set\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "\n",
        "        images = images.to(\"cpu\")  \n",
        "        labels = labels.to(\"cpu\") \n",
        "        model_cpu = model.to(\"cpu\") \n",
        "        \n",
        "        outputs = model(images)\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "    \n",
        "model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "data, _ = ins_dataset_train[0]\n",
        "data.unsqueeze_(0)\n",
        "output = model(data)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "model_cpu.conv1.register_forward_hook(get_activation('conv1'))\n",
        "data, _ = ins_dataset_train[0]\n",
        "data = data.to(\"cpu\")\n",
        "data.unsqueeze_(0)\n",
        "output = model_cpu(data)\n",
        "\"\"\"\n",
        "\n",
        "act = activation['conv1'].squeeze()\n",
        "fig, axarr = plt.subplots(2,2)\n",
        "for idx in range(act.size(0)):\n",
        "\n",
        "    axarr[0,0].imshow(act[0], cmap=\"gray\")\n",
        "    axarr[0,1].imshow(act[1], cmap=\"gray\")\n",
        "    axarr[1,0].imshow(act[2], cmap=\"gray\")\n",
        "    axarr[1,1].imshow(act[3], cmap=\"gray\")\\\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFyMDEHG9qy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    # Iterate over the test set\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "\n",
        "        images = images.to(\"cpu\")  \n",
        "        labels = labels.to(\"cpu\") \n",
        "        model_cpu = model.to(\"cpu\") \n",
        "        \n",
        "        outputs = model(images)\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "model.conv2.register_forward_hook(get_activation('conv2'))\n",
        "data, _ = ins_dataset_train[0]\n",
        "data.unsqueeze_(0)\n",
        "output = model(data)\n",
        "\n",
        "act = activation['conv2'].squeeze()\n",
        "fig, axarr = plt.subplots(2,2)\n",
        "for idx in range(act.size(0)):\n",
        "    axarr[0,0].imshow(act[0], cmap=\"gray\")\n",
        "    axarr[0,1].imshow(act[1], cmap=\"gray\")\n",
        "    axarr[1,0].imshow(act[2], cmap=\"gray\")\n",
        "    axarr[1,1].imshow(act[3], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjF09LBK-kgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    # Iterate over the test set\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "\n",
        "        images = images.to(\"cpu\")  \n",
        "        labels = labels.to(\"cpu\") \n",
        "        model_cpu = model.to(\"cpu\") \n",
        "        \n",
        "        outputs = model(images)\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "model.conv3.register_forward_hook(get_activation('conv3'))\n",
        "data, _ = ins_dataset_train[0]\n",
        "data.unsqueeze_(0)\n",
        "output = model(data)\n",
        "\n",
        "act = activation['conv3'].squeeze()\n",
        "fig, axarr = plt.subplots(2,2)\n",
        "for idx in range(act.size(0)):\n",
        "    axarr[0,0].imshow(act[0], cmap=\"gray\")\n",
        "    axarr[0,1].imshow(act[1], cmap=\"gray\")\n",
        "    axarr[1,0].imshow(act[2], cmap=\"gray\")\n",
        "    axarr[1,1].imshow(act[3], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mejEJ18O-sZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    # Iterate over the test set\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "\n",
        "        images = images.to(\"cpu\")  \n",
        "        labels = labels.to(\"cpu\") \n",
        "        model_cpu = model.to(\"cpu\") \n",
        "        \n",
        "        outputs = model(images)\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "model.conv4.register_forward_hook(get_activation('conv4'))\n",
        "data, _ = ins_dataset_train[0]\n",
        "data.unsqueeze_(0)\n",
        "output = model(data)\n",
        "\n",
        "act = activation['conv4'].squeeze()\n",
        "fig, axarr = plt.subplots(2,2)\n",
        "for idx in range(act.size(0)):\n",
        "    axarr[0,0].imshow(act[0], cmap=\"gray\")\n",
        "    axarr[0,1].imshow(act[1], cmap=\"gray\")\n",
        "    axarr[1,0].imshow(act[2], cmap=\"gray\")\n",
        "    axarr[1,1].imshow(act[3], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnp9B6x7-zQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    # Iterate over the test set\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "\n",
        "        images = images.to(\"cpu\")  \n",
        "        labels = labels.to(\"cpu\") \n",
        "        model_cpu = model.to(\"cpu\") \n",
        "        \n",
        "        outputs = model(images)\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "model.conv5.register_forward_hook(get_activation('conv5'))\n",
        "data, _ = ins_dataset_train[0]\n",
        "data.unsqueeze_(0)\n",
        "output = model(data)\n",
        "\n",
        "act = activation['conv5'].squeeze()\n",
        "fig, axarr = plt.subplots(2,2)\n",
        "for idx in range(act.size(0)):\n",
        "    axarr[0,0].imshow(act[0], cmap=\"gray\")\n",
        "    axarr[0,1].imshow(act[1], cmap=\"gray\")\n",
        "    axarr[1,0].imshow(act[2], cmap=\"gray\")\n",
        "    axarr[1,1].imshow(act[3], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}